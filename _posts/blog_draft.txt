Whether your are a newbie or professional in

Text preprocessing is one of the tasks that most of those working with text files in natural language processing (NLP) or data analytics go through frequently. Text pre-processing is a normalisation step which is an inseparable component of text processing tasks such as

 text by removing special tokens, adding


 {% highlight bash %}
 	(France, capital, Paris)
 	(Paris, population, 2141000)
 {% endhighlight %}

- Creating your own spell-checker using Hunspell
- A simple yet robust pipeline for language technology for less-resourced languages
- SPARQL
- EndonymLinked
- LinkedBib: import bibliography through linked data and query within LaTeX
- NLP in the ME
- practical nlp from scratch


### Basic text statistics

wc [-clmw] [file ...]

A word count command could not have a more explicit name than `wc`. `wc` is a very practical command which counts line, word and character. In addition, there are a bunch of other options which may be used with. The following command summarises `wc` and the functions of its options:

<table align="center" class="table table-bordered table-hover table-condensed">
<tr>
<th title="Field #1">Option</th>
<th title="Field #2">Function</th>
</tr>
<tr >
<td><b>-c</b></td>
<td>print the number of bytes. This will cancel out any prior usage of the -m option.</td>
</tr>
<tr>
<td><b>-m</b></td>
<td>print the character counts.</td>
</tr>
<tr>
<td><b>-l</b></td>
<td>print the newline counts.</td>
</tr>
</table>

## Converting from character cases

tr '[:upper:]' '[:lower:]' < input

## Find the n most frequent words

tr -c '[:alnum:]' '[\n*]' < test.txt | sort | uniq -c | sort -nr | head  -10

If you don't put in a sort before the uniq -c you'll probably get a lot of false singleton words. uniq only does unique runs of lines, not overall uniquness.

## Replace punctuation marks 

cat head.txt | sed 's/[[:punct:]]/ /g'

## Strip multiple spaces to one using sed

 sed 's/  */ /g'

## Split sentences by line in a text

sed -e :1 -e 's/\([.?!]\)[[:blank:]]\{1,\}\([^[:blank:]]\)/\1\\2/;t1'

### Remove metadata of PDF

Although anonymization of files is not that much related to the command-line, we also discuss as it is a part of pre-processing.

/Users/sina/My_GitLab/finnlp/French_financial_corpus/CAC40/Axa/0A/DR/AXA-DDR2017-FR-PDF-e-accessible_02.pdf


https://apple.stackexchange.com/questions/87313/how-to-remove-xattr-com-apple-quarantine-from-all-webarchive-files-with-that-ex
https://apple.stackexchange.com/questions/110239/where-is-the-where-from-meta-data-stored-when-downloaded-via-chrome
http://www.linux-magazine.com/Online/Blogs/Productivity-Sauce/Remove-EXIF-Metadata-from-Photos-with-exiftool


Find all file of all types
find . -iname '*.pdf'

http://u88.n24.queensu.ca/exiftool/forum/index.php?topic=3943.0


A tool for lexicographers to manually create resources in OntoLex.
A converter from Tabular format to Ontolex
all about language modelling
all about word embeddings


---
show_profile: false
title: Converting tabular data to Lexicog in Ontolex-Lemon
show_profile: false
tags:
- Ontolex-Lemon
- Lexicography
- Resource
- Linked data
- Data conversion
---

This blog post describes the conversion tool that I created to convert tabular data into Ontolex-Lemon in RDF. Find the repository at [https://github.com/sinaahmadi/Tabular2Lexicog](https://github.com/sinaahmadi/Tabular2Lexicog){:target="_blank"}.

---
